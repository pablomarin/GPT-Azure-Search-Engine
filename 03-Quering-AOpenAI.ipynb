{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59d527f-1100-45ff-b051-5f7c9029d94d",
   "metadata": {},
   "source": [
    "# Queries with and without Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba",
   "metadata": {},
   "source": [
    "So far, you have your Search Engine loaded **from two different data sources in two diferent indexes**, on this notebook we are going to try some example queries and then use Azure OpenAI service to see if we can get a good answer for the user query.\n",
    "\n",
    "The idea is that a user can ask a question about Computer Science (first datasource/index) or about Covid (second datasource/index), and the engine will respond accordingly.\n",
    "This **Multi-Index** demo, mimics the scenario where a company loads multiple type of documents of different types and about completly different topics and the search engine must respond with the most relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from typing import List\n",
    "from operator import itemgetter\n",
    "\n",
    "# LangChain Imports needed\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "\n",
    "# Our own libraries needed\n",
    "from common.prompts import DOCSEARCH_PROMPT\n",
    "from common.utils import get_search_results\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297d29b-1f61-4dce-858e-bf4272172dba",
   "metadata": {},
   "source": [
    "## Multi-Index Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a46e2d3-298a-4708-83de-9e108b1a117a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text-based Indexes that we are going to query (from Notebook 01 and 02)\n",
    "index1_name = \"srch-index-files\"\n",
    "index2_name = \"srch-index-csv\"\n",
    "indexes = [index2_name, index1_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62ebb2-d7be-4bfb-b1ba-4db86c11839a",
   "metadata": {},
   "source": [
    "Try questions that you think might be answered or addressed in computer science papers in 2020-2021 or that can be addressed by medical publications about COVID in 2020-2021. Try comparing the results with the open version of ChatGPT.<br>\n",
    "The idea is that the answers using Azure OpenAI only looks at the information contained on these publications.\n",
    "\n",
    "**Example Questions you can ask**:\n",
    "- Is Chandler ever jealous of Richard?\n",
    "- Who is Mindy?\n",
    "- What happened between Ross and Rachel in Vegas?\n",
    "- What are some examples of reinforcement learning in virus spread?\n",
    "- What are the main risk factors for Covid-19?\n",
    "- What medicine reduces inflamation in the lungs?\n",
    "- Why Covid doesn't affect kids that much compared to adults?\n",
    "- Does chloroquine really works against covid?\n",
    "- Who won the 1994 soccer world cup? # This question should yield no answer if the system is correctly grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUESTION = \"Is Chandler ever jealous of Richard?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d925eb-7f9c-429e-a62a-4c37d7702caf",
   "metadata": {},
   "source": [
    "### Search on both indexes individually and aggragate results\n",
    "\n",
    "#### **Note**: \n",
    "In order to standarize the indexes, **there must be 6 mandatory fields present on each index**: `id, title, name, location, chunk, chunkVector`. This is so that each document can be treated the same along the code. Also, **all indexes must have a semantic configuration**.\n",
    "\n",
    "We are going to use Hybrid Queries: Text + Vector Search combined for optimal results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf2e30f-e71f-4533-ab52-27d048b80a89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Index: srch-index-csv Results Found: 69681, Results Returned: 10\n",
      "200\n",
      "Index: srch-index-files Results Found: 2896, Results Returned: 10\n"
     ]
    }
   ],
   "source": [
    "agg_search_results = dict()\n",
    "k = 10\n",
    "\n",
    "for index in indexes:\n",
    "    search_payload = {\n",
    "        \"search\": QUESTION, # Text query\n",
    "        \"select\": \"id, title, name, location, chunk\",\n",
    "        \"queryType\": \"semantic\",\n",
    "        \"vectorQueries\": [{\"text\": QUESTION, \"fields\": \"chunkVector\", \"kind\": \"text\", \"k\": k}], # Vector query\n",
    "        \"semanticConfiguration\": \"my-semantic-config\",\n",
    "        \"captions\": \"extractive\",\n",
    "        \"answers\": \"extractive\",\n",
    "        \"count\":\"true\",\n",
    "        \"top\": k\n",
    "    }\n",
    "\n",
    "    r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index + \"/docs/search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
    "    print(r.status_code)\n",
    "\n",
    "    search_results = r.json()\n",
    "    agg_search_results[index]=search_results\n",
    "    print(\"Index:\", index, \"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33018be-350d-4c54-b491-a86bc1cfffb6",
   "metadata": {},
   "source": [
    "#### Important Note: \n",
    "You may encounter errors when attempting to search for results IF the indexer is still processing documents. This occurs because the embedding model is heavily utilized by the indexer, hitting its TPM quota. If you experience search errors, please try again or wait until the indexing is complete, which may take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255c40f5-d836-480c-8c68-06a2282c8146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agg_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd0fe5-4ee0-42e2-a920-72b93a407389",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display the top results (from both searches) based on the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e938337-602d-4b61-8141-b8c92a5d91da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Top Answers</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>Top Results</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s09/e07/c11.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s09/e07/c11.txt</a> - score: 2.36</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Chandler Bing: Oh, yeah, well, poor Richard Y' I can grow a moustache! Monica Geller: Chandler, this is not our problem We've got each other That's all that matters Chandler Bing: Yeah, oh, but I just keep picturing you rolling around with him with your cowboy boots in the air Monica Geller: Cowboy boots? I've never worn cowboy boots in my whole li..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s09/e23/c02.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s09/e23/c02.txt</a> - score: 2.0</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Monica Geller: I mean, my feelings for Richard are certainly gone Phoebe Buffay: You just did it again Chandler, your feelings for Chandler are certainly gone!.\u0000 I mean, you know, Monica refers to Chandler as Richard all the time! Chandler Bing: She does? David: Oh, certainly That's a combination of Bernoulli's principle and Newton's third law of m..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s03/e12/c15.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s03/e12/c15.txt</a> - score: 1.92</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "I mean doesn't she have any y'know other stripper moms friends of her own? Ross Geller: You are totally jealous Rachel Green: I'm not jealous All right this is about, umm, people feeling certain things y'know about strippers And y'know, and um, I Ross Geller: Honey, I love you too Rachel Green: Ugh Wait, wait, wait Ross Geller: What?  unknown: nan ..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c01.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c01.txt</a> - score: 1.9</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "And I know I probably shouldn't even be here telling you this, I mean you're with Chandler a guy I really like, and if you say he's straight I'll believe you! After seeing ya the other night I knew if I didn't tell ya I'd regret it for the rest of my life Letting you go was the stupidest thing I ever did Monica Geller: Y'know you're really not supp..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c12.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c12.txt</a> - score: 1.86</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "my God.  unknown: nan  Monica Geller: Chandler In all my life I never thought I would be so lucky As to...fall in love with my best...my best There's a reason why girls don't do this! Chandler Bing: Okay! Okay! Okay! Oh God, I thought Wait a minute, I-I can do this I thought that it mattered what I said or where I said it Then I realized the only t..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s05/e23/c03.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s05/e23/c03.txt</a> - score: 1.85</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Phoebe Buffay: Oh no.  Chandler Bing: What Richard thing? Phoebe Buffay: Simmons! Go with Simmons! Monica Geller: Okay, I umm, I ran into Richard yesterday and he asked me if I wanted to go for a bite and I did The only reason I didn't tell you is because I knew you'd get mad and I didn't want to spoil our anniversary Chandler Bing: I'm not mad Mon..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s07/e02/c08.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s07/e02/c08.txt</a> - score: 1.83</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Jack Geller: We started saving again when you were dating Richard and then that went to hell, so we redid the kitchen Monica Geller: What about when I started dating Chandler? Judy Geller: Well it was Chandler! We didn't think he'd ever propose! Chandler Bing: Clearly I did not start drinking enough at the start of the meal Monica Geller: I can't b..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s05/e22/c01.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s05/e22/c01.txt</a> - score: 1.78</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "That's all I ever hear, Richard, Richard, Richard! Monica Geller: Since we've been going out, I think I've mentioned his name twice! Chandler Bing: Okay, so Richard, Richard! Monica Geller: It's not Richard! Okay? It's this new guy and he's really good Rachel Green: Well, I'm sorry I'm not going to an eye doctor! Ross Geller: Oh God, here we go! Ch..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c08.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c08.txt</a> - score: 1.73</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Well, I'll just leave the door open and go sit on the couch Monica Geller: Chandler is such an idiot! Richard Burke: Drink? Monica Geller: Yeah, I'll have a scotch Richard Burke: ...on the rocks with a twist? I remember Monica Geller: Still smoking cigars? Richard Burke: Uh, no! No! That's...art! If it bothers you I can put my art out Monica Geller..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e24/c07.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D\">https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e24/c07.txt</a> - score: 1.72</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "I'm Chandler! Oh, that's Richard! Monica Geller: Oh God, maybe he won't see us Richard!  unknown: nan  Richard Burke: Monica! Chandler! Chandler Bing: Hey-hey, hey! I don't know why I did that! Monica Geller: Hey, it's good to see you! Richard Burke: You too, you let uh, your hair grow long Monica Geller: Yeah-Oh that's right You, you always wanted..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h4>Top Answers</h4>'))\n",
    "\n",
    "for index,search_results in agg_search_results.items():\n",
    "\n",
    "    for result in search_results['@search.answers']:\n",
    "        if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
    "            display(HTML('<h5>' + 'Answer - score: ' + str(round(result['score'],2)) + '</h5>'))\n",
    "            display(HTML(result['text']))\n",
    "\n",
    "            \n",
    "print(\"\\n\\n\")\n",
    "display(HTML('<h4>Top Results</h4>'))\n",
    "\n",
    "content = dict()\n",
    "ordered_content = OrderedDict()\n",
    "\n",
    "\n",
    "for index,search_results in agg_search_results.items():\n",
    "    for result in search_results['value']:\n",
    "        if result['@search.rerankerScore'] > 1:# Show answers that are at least 25% of the max possible score=4\n",
    "            content[result['id']]={\n",
    "                                    \"title\": result['title'],\n",
    "                                    \"chunk\": result['chunk'], \n",
    "                                    \"name\": result['name'], \n",
    "                                    \"location\": result['location'] ,\n",
    "                                    \"caption\": result['@search.captions'][0]['text'],\n",
    "                                    \"score\": result['@search.rerankerScore'],\n",
    "                                    \"index\": index\n",
    "                                    }\n",
    "    \n",
    "#After results have been filtered we will Sort and add them as an Ordered list\\n\",\n",
    "for id in sorted(content, key= lambda x: content[x][\"score\"], reverse=True):\n",
    "    ordered_content[id] = content[id]\n",
    "    url = str(ordered_content[id]['location']) + os.environ['BLOB_SAS_TOKEN']\n",
    "    title = str(ordered_content[id]['title']) if (ordered_content[id]['title']) else ordered_content[id]['name']\n",
    "    score = str(round(ordered_content[id]['score'],2))\n",
    "    display(HTML('<h5><a href=\"'+ url + '\">' + ordered_content[id]['location'] + '</a> - score: '+ score + '</h5>'))\n",
    "    display(HTML(ordered_content[id]['caption']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5",
   "metadata": {},
   "source": [
    "### Comments on Query results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4",
   "metadata": {},
   "source": [
    "As seen above the semantic re-ranking feature of Azure AI Search service is good. It gives answers (sometimes) and also the top results with the corresponding file and the paragraph where the answers is possible located.\n",
    "\n",
    "Let's see if we can make this better with Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec",
   "metadata": {},
   "source": [
    "# Using Azure OpenAI\n",
    "\n",
    "To use OpenAI to get a better answer to our question, the thought process is simple: let's **give the answer and the content of the documents from the search result to the GPT model as context and let it provide a better response**. This is what RAG (Retreival Augmented Generation) is about.\n",
    "\n",
    "Now, before we do this, we need to understand a few things first:\n",
    "\n",
    "1) Chainning and Prompt Engineering\n",
    "2) Embeddings\n",
    "\n",
    "We will use a library call **LangChain** that wraps a lot of boiler plate code.\n",
    "Langchain is one library that does a lot of the prompt engineering for us under the hood, for more information see [here](https://python.langchain.com/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d9138-2250-4f6b-bc88-50d7957f8d33",
   "metadata": {},
   "source": [
    "**Important Note**: Starting now, we will utilize OpenAI models. Please ensure that you have deployed the following models within the Azure OpenAI portal:\n",
    "\n",
    "- text-embedding-ada-002 (or newer)\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "\n",
    "Reference for Azure OpenAI models (regions, limits, dimensions, etc): [HERE](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c720e-ece1-45ad-9d01-2dfd15c182bb",
   "metadata": {},
   "source": [
    "## A gentle intro to chaining LLMs and prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd7028-5a6c-4296-8c85-4f420d408d69",
   "metadata": {},
   "source": [
    "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step.\n",
    "\n",
    "Azure OpenAI is a type of LLM (provider) that you can use but there are others like Cohere, Huggingface, etc.\n",
    "\n",
    "Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
    "\n",
    "* Generic — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).\n",
    "\n",
    "Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13df9247-e784-4e04-9475-55e672efea47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMPLETION_TOKENS = 2000\n",
    "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT4oMINI_DEPLOYMENT_NAME\"], \n",
    "                      temperature=0, \n",
    "                      max_tokens=COMPLETION_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b55adb-6f98-4f15-b67a-9fbba5820560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant that give thorough responses to users.\"),\n",
    "    (\"user\", \"{input}. Give your response in {language}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417d052-0035-4635-93e8-2bd3ec50d796",
   "metadata": {},
   "source": [
    "The | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a37e60-a1ef-4750-a1ec-9e4fe5ba07fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6be6b4df-ee2c-4a0c-8ad3-a672d70f4f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Chandler does experience feelings of jealousy towards Richard in the TV show \"Friends.\" This jealousy primarily arises during the earlier seasons when Monica starts dating Richard, who is significantly older than her. Chandler, being one of Monica's close friends, feels protective of her and is concerned about the age difference and the seriousness of their relationship.\n",
       "\n",
       "In particular, Chandler's jealousy is highlighted in episodes where he expresses discomfort with Richard's presence and the way Monica seems to be deeply invested in the relationship. For example, in Season 2, Episode 24 (\"The One with Barry and Mindy\"), Chandler's jealousy is evident when he makes sarcastic comments about Richard and tries to undermine their relationship.\n",
       "\n",
       "Overall, Chandler's jealousy is a mix of protectiveness for Monica and insecurity about his own romantic life, especially as he navigates his own relationships throughout the series. This dynamic adds depth to Chandler's character and showcases the complexities of friendships and romantic relationships within the group."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 ms, sys: 610 μs, total: 18.1 ms\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "display(Markdown(chain.invoke({\"input\": QUESTION, \"language\": \"English\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8539d0-a538-4368-82c3-5f91d8370f1e",
   "metadata": {},
   "source": [
    "**Note**: this is the first time you use OpenAI in this Accelerator, so if you get a Resource not found error, is most likely because the name of your OpenAI model deployment is different than the environmental variable set above `os.environ[\"GPT4oMINI_DEPLOYMENT_NAME\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed014c-0c6b-448c-b995-fe7970b92ad5",
   "metadata": {},
   "source": [
    "Great!!, now you know how to create a simple prompt and use a chain in order to answer a general question using ChatGPT knowledge!. \n",
    "\n",
    "It is important to note that we rarely use generic chains as standalone chains. More often they are used as building blocks for Utility chains (as we will see next). Also important to notice is that we are NOT using our documents or the result of the Azure Search yet, just the knowledge of ChatGPT on the data it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c48038-b1af-4228-8ffb-720e554fd3b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**The second type of Chains are Utility:**\n",
    "\n",
    "* Utility — These are specialized chains, comprised of many building blocks to help solve a specific task. For example, LangChain supports some end-to-end chains (such as `create_retrieval_chain` for QnA Doc retrieval, Summarization, etc).\n",
    "\n",
    "We will build our own specific chain in this workshop for digging deeper and solve our use case of enhancing the results of Azure AI Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0454ddb-44d8-4fa9-929a-5e5563dd28f8",
   "metadata": {},
   "source": [
    "\n",
    "But before dealing with the utility chain needed, let's first review the concept of Embeddings and Vector Search and RAG. \n",
    "\n",
    "## Embeddings and Vector Search\n",
    "\n",
    "From the Azure OpenAI documentation ([HERE](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings?tabs=python)), An embedding is a special format of data representation that can be easily utilized by machine learning models and algorithms. The embedding is an information dense representation of the semantic meaning of a piece of text. Each embedding is a vector of floating point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar. \n",
    "\n",
    "### Why Do We Need Vectors?\n",
    "\n",
    "Vectors are essential for several reasons:\n",
    "\n",
    "- **Semantic Richness**: They convert the semantic meaning of text into mathematical vectors, capturing nuances that simple keyword searches miss. This makes them incredibly powerful for understanding and processing language.\n",
    "- **Human-like Searching**: Searching using vector distances mimics the human approach to finding information based on context and meaning, rather than relying solely on exact word matches.\n",
    "- **Efficiency in Scale**: Vector representations allow for efficient handling and searching of large datasets. By reducing complex text to numerical vectors, algorithms can quickly sift through vast amounts of information.\n",
    "\n",
    "### Understanding LLM Tokens' Context Limitation\n",
    "\n",
    "Large Language Models (LLMs) like GPT come with a token limit for each input, which poses a challenge when dealing with lengthy documents or extensive data sets. This limitation restricts the model's ability to understand and generate responses based on the full context of the information provided. It becomes crucial, therefore, to devise strategies that can effectively manage and circumvent this limitation to leverage the full power of LLMs.\n",
    "\n",
    "To address this challenge, the solution incorporates several key steps:\n",
    "\n",
    "1. **Segmenting Documents**: Breaking down large documents into smaller, manageable segments.\n",
    "2. **Vectorization of Chunks**: Converting these segments into vectors, making them compatible with vector-based search techniques.\n",
    "3. **Hybrid Search**: Employing both vector and text search methods to pinpoint the most relevant segments in relation to the query.\n",
    "4. **Optimal Context Provision**: Presenting the LLM with the most pertinent segments, ensuring a balance between detail and brevity to stay within token limits.\n",
    "\n",
    "\n",
    "Our ultimate goal is to rely solely on vector indexes and hybrid searchs (vector + text). While it is possible to manually code parsers with OCR for various file types and develop a scheduler to synchronize data with the index, there is a more efficient alternative: **Azure AI Search has automated chunking strategies and vectorization**.\n",
    "\n",
    "It's important to note that **document segmentation and vectorization have already been completed in AI Azure Search**, as seen in the `ordered_content` dictionary. This pre-processing step simplifies subsequent operations, ensuring rapid response times and adherence to the token limits of the chosen OpenAI model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e79235-3d8b-4713-9336-5004cc4a1556",
   "metadata": {},
   "source": [
    "So really, our only job now is to make sure that the results from the Azure AI Search queries fit on the LLM context size, and then let it do its magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12682a1b-df92-49ce-a638-7277103f6cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name = \"srch-index-files\"\n",
    "index2_name = \"srch-index-csv\"\n",
    "indexes = [index1_name, index2_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6d6a7-18ef-45b2-a216-3c1f50006593",
   "metadata": {},
   "source": [
    "In order to not duplicate code, we have put many of the code used above into functions. These functions are in the `common/utils.py` and `common/prompts.py` files. This way we can use these functios in the app that we will build later.\n",
    "\n",
    "`get_search_results()` do the multi-index search and returns the combined ordered list of documents/chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bccca45-d1dd-476f-b109-a528b857b6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 20\n"
     ]
    }
   ],
   "source": [
    "k = 20  # play with this parameter and see the quality of the final answer\n",
    "ordered_results = get_search_results(QUESTION, indexes, k=k, reranker_threshold=1)\n",
    "print(\"Number of results:\",len(ordered_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7714f38a-daaa-4fc5-a95a-dd025d153216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the below line if you want to inspect the ordered results\n",
    "# ordered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d4238-df6e-40eb-ab38-4fe6db614acd",
   "metadata": {},
   "source": [
    "Now let's create a Prompt Template that will ground the response only in the chunks retrieve by our hybrid AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f86ed786-aca0-4e25-947b-d9cf3a82665c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question thoroughly, based **ONLY** on the following context:\n",
    "{context}\n",
    "\n",
    "Important: Assume you know nothing about the subject, only based your answer on the context above.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25cba3d1-b5ab-4e28-96b3-ef923d99dc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Chandler Bing does exhibit jealousy towards Richard Burke in the context provided. In one exchange, Chandler expresses discomfort and jealousy when he imagines Monica being intimate with Richard, saying, \"I just keep picturing you rolling around with him with your cowboy boots in the air.\" This indicates that Chandler is affected by the idea of Monica's past relationship with Richard and feels insecure about it. Additionally, Chandler's reaction to Monica having lunch with Richard further suggests that he is not entirely comfortable with her interactions with him, as he questions why she didn't inform him about it. Overall, Chandler's comments and reactions reflect a sense of jealousy regarding Richard."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 ms, sys: 660 μs, total: 18.9 ms\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Creation of our custom chain\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "try:\n",
    "    display(Markdown(chain.invoke({\"question\": QUESTION, \"context\": ordered_results})))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417925af-486a-40bc-a290-28c35968c581",
   "metadata": {},
   "source": [
    "# Improving the Prompt and adding citations\n",
    "\n",
    "We could see above that in the answer given by GPT4o-mini, there is no citations or references. **How do we know if the answer is grounded on the context or not?**\n",
    "\n",
    "Let's see if this can be improved by Prompt Engineering.<br>\n",
    "On `common/prompts.py` we created a prompt called `DOCSEARCH_PROMPT` check it out!\n",
    "\n",
    "**Let's also create a custom Retriever class** so we can plug it in easily within the chain building. \n",
    "Note: we can also use the Azure AI Search retriever class [HERE](https://python.langchain.com/docs/integrations/vectorstores/azuresearch), however we want to create a custom Retriever for the following reasons:\n",
    "\n",
    "1) We want to do multi-index searches in one call\n",
    "2) Easier to teach complex concepts of LangChain in this notebook\n",
    "3) We want to use the REST API vs the Python Azure Search SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdf31f99-0dfb-423a-81f5-03018e61d9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    \n",
    "    topK : int\n",
    "    reranker_threshold : int\n",
    "    indexes: List\n",
    "    sas_token: str = None\n",
    "    search_filter: str = None\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \n",
    "        ordered_results = get_search_results(query, self.indexes, k=self.topK, \n",
    "                                             reranker_threshold=self.reranker_threshold, \n",
    "                                             sas_token=self.sas_token, search_filter=self.search_filter)\n",
    "        top_docs = []\n",
    "        for key,value in ordered_results.items():\n",
    "            location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "            top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location, \"score\":value[\"score\"]}))\n",
    "\n",
    "        return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b39c79-c827-4437-b58b-6a6fae53b968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retriever = CustomRetriever(indexes=indexes, topK=k, reranker_threshold=1, sas_token=os.environ['BLOB_SAS_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7aa4f58-4791-40a0-80c5-6582e0574579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test retreiver\n",
    "results = retriever.invoke(QUESTION)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11b6546f-b5c5-4168-97fc-2636c50e41c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can create now a dynamically configurable llm object that can change the model at runtime\n",
    "dynamic_llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT4oMINI_DEPLOYMENT_NAME\"], \n",
    "                              temperature=0.5, max_tokens=COMPLETION_TOKENS).configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM  (initialized above) will be used\n",
    "    default_key=\"gpt4omini\",\n",
    "    # This adds a new option, with name `gpt4o`\n",
    "    gpt4o=AzureChatOpenAI(deployment_name=os.environ[\"GPT4o_DEPLOYMENT_NAME\"], \n",
    "                         temperature=0.5, max_tokens=COMPLETION_TOKENS),\n",
    "    # You can add more configuration options here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7da2f31-cf5d-4f3a-aad5-67b50b56968e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declaration of the chain with the dynamic llm and the new prompt\n",
    "configurable_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, # Passes the question to the retriever and the results are assign to context\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | DOCSEARCH_PROMPT  # Passes the input variables above to the prompt template\n",
    "    | dynamic_llm   # Passes the finished prompt to the LLM\n",
    "    | StrOutputParser()  # converts the output (Runnable object) to the desired output (string)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b67200e5-d3ae-4c86-9f69-bc7b964ab532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Chandler Bing does exhibit feelings of jealousy towards Richard Burke in several instances. \n",
       "\n",
       "1. **Initial Jealousy**: Chandler expresses jealousy when he sees Monica interacting with Richard. He comments on how he keeps picturing Monica with Richard, indicating that it bothers him [[1]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s09/e07/c11.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D).\n",
       "\n",
       "2. **Confrontation with Richard**: When Chandler confronts Richard, he expresses frustration that Richard is making Monica think about their relationship. Chandler says, \"You made my girlfriend think!!\" which indicates that he feels threatened by Richard's presence in Monica's life [[2]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c11.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D).\n",
       "\n",
       "3. **Discussion of Marriage**: Monica mentions that Richard wants to marry her, which further amplifies Chandler's insecurities about his own relationship with her. Chandler's reaction to this news shows that he is indeed jealous of Richard's intentions [[3]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c05.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D).\n",
       "\n",
       "These moments illustrate that Chandler does feel jealousy towards Richard, particularly regarding his relationship with Monica and the prospect of marriage."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 ms, sys: 488 μs, total: 41.4 ms\n",
      "Wall time: 5.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    display(Markdown(configurable_chain.with_config(configurable={\"model\": \"gpt4omini\"}).invoke({\"question\": QUESTION})))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661b48d-1e57-4a70-9b0a-cc59f9093267",
   "metadata": {},
   "source": [
    "As seen above, we were able to improve the quality and breath of the answer and add citations with only prompt engineering!\n",
    "\n",
    "#### Let's try again, but with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efcfac6b-bac2-40c6-9ded-e4ee38e3093f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Chandler is jealous of Richard on multiple occasions. In one instance, Chandler expresses his jealousy when he discovers that Richard keeps a tape of Monica, implying Richard isn't over her. Chandler feels insecure about Richard's ability to handle such things maturely and is bothered by the idea that Richard might still have feelings for Monica [[1]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s09/e07/c11.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D).\n",
       "\n",
       "Additionally, Chandler's jealousy is evident when Monica has lunch with Richard and doesn't tell him, which leads to an argument. Chandler is upset about Monica having secret interactions with Richard, highlighting his insecurities and jealousy [[2]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s05/e23/c06.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 ms, sys: 0 ns, total: 38.9 ms\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    display(Markdown(configurable_chain.with_config(configurable={\"model\": \"gpt4o\"}).invoke({\"question\": QUESTION})))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c791651-2c56-4de7-a232-8ff94937b938",
   "metadata": {},
   "source": [
    "**Answers from GPT-4o-mini and GPT-4o doesn't seem to be much different!**\n",
    "This means that for simple tasks of following instructions and a context, both models seem to behave similarly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6690453b-a9b1-4907-bd43-8c6b3ecba26e",
   "metadata": {},
   "source": [
    "## Adding Streaming to improve user experience and performance\n",
    "\n",
    "One way to make the response look faster is to stream the answer, so the user can see the response as it is typed. To do this, we just simply need to call the method `stream` instead of `invoke`. More on Streaming and Callbacks in later notebooks, but for now, this is one simple way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d250c88-5984-438f-8390-1d93756048ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Chandler is jealous of Richard on multiple occasions. In one instance, Chandler is upset because he believes Richard is not over Monica, as Richard keeps a tape that Chandler thinks features Monica. Chandler expresses his insecurity by imagining Monica with Richard, which bothers him until he realizes the tape isn't of Monica [[1]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s09/e07/c11.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D).\n",
      "\n",
      "Additionally, Chandler feels insecure when Monica has lunch with Richard and doesn't tell him. He is upset about Monica's secrecy and the fact that she met with Richard, leading to a misunderstanding between them [[2]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s05/e23/c06.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D).\n",
      "\n",
      "Lastly, Chandler is concerned about Richard's presence when Richard confesses his love for Monica. Chandler feels threatened and is worried that Monica might choose Richard over him [[3]](https://blobstorageuq7x4ufcftcpm.blob.core.windows.net/friends/s06/e25/c11.txt?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-10-03T01:44:00Z&st=2024-10-02T17:44:00Z&spr=https&sig=0eJm6CaaACeHGfgKGIXE163moq7X0Mu6tbZcCU0MHkA%3D)."
     ]
    }
   ],
   "source": [
    "for chunk in configurable_chain.with_config(configurable={\"model\": \"gpt4o\"}).stream({\"question\": QUESTION}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bf690-3398-4824-96bc-b28e4da7e524",
   "metadata": {},
   "source": [
    "## Testing Groundness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb4f9e-4fec-4e23-8d89-be0cf900e34d",
   "metadata": {},
   "source": [
    "Let’s ask the same question again, but this time for Season 1, where we know Richard doesn’t appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad7644c3-e92e-4e6c-9a3e-a64f6f036be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_filter = \"substringof('/s01/', location)\"\n",
    "retriever = CustomRetriever(indexes=indexes, topK=k, reranker_threshold=1, \n",
    "                            sas_token=os.environ['BLOB_SAS_TOKEN'],\n",
    "                            search_filter=search_filter)\n",
    "configurable_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, # Passes the question to the retriever and the results are assign to context\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | DOCSEARCH_PROMPT  # Passes the input variables above to the prompt template\n",
    "    | dynamic_llm   # Passes the finished prompt to the LLM\n",
    "    | StrOutputParser()  # converts the output (Runnable object) to the desired output (string)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69e78ed8-e03e-4b9b-a9d6-d4fbd9563b66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Empty Search Response\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I don't have any information on that topic based on the provided context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 ms, sys: 6.77 ms, total: 38.9 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    display(Markdown(configurable_chain.with_config(configurable={\"model\": \"gpt4o\"}).invoke({\"question\": QUESTION})))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c84a9-d4bc-44f0-9270-2896e756e14c",
   "metadata": {},
   "source": [
    "**Perfect!**, even know the model knows the answer based on its training data, it is grounding the answer only on the results from the context retrieved from azure AI search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### By using OpenAI, the answers to user questions are way better than taking just the results from Azure AI Search. So the summary is:\n",
    "- Utilizing Azure AI Search, we conduct a multi-index hybrid search that identifies the top chunks of documents from each index.\n",
    "- Subsequently, Azure OpenAI utilizes these extracted chunks as context, comprehends the content, and employs it to deliver optimal answers.\n",
    "- Best of two worlds!\n",
    "\n",
    "##### Important observations on this notebook:\n",
    "\n",
    "1) Answers with GPT-4o-mini and GPT-4o seems to be very similar\n",
    "2) Both models provide good and diverse citations in the right format\n",
    "3) Streaming the answers improves the user experience big time!\n",
    "4) We achieved high levels of groundness using prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6e2fe-1c34-4952-99ad-14940f022379",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "In the next notebook, we are going to see how we can treat complex and large documents separately, also using Vector Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
