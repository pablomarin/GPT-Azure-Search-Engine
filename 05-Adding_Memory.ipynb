{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
   "metadata": {},
   "source": [
    "# Understanding Memory in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
   "metadata": {},
   "source": [
    "In the previous Notebook, we successfully explored how OpenAI models can enhance the results from Azure Cognitive Search. \n",
    "\n",
    "However, we have yet to discover how to engage in a conversation with the LLM. With [Bing Chat](http://chat.bing.com/), for example, this is possible, as it can understand and reference the previous responses.\n",
    "\n",
    "There is a common misconception that GPT models have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
    "\n",
    "In this Notebook, our goal is to illustrate how we can effectively \"endow the LLM with memory\" by employing prompts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferMemory, ConversationTokenBufferMemory\n",
    "from openai.error import OpenAIError\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import CosmosDBChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import (\n",
    "    get_search_results,\n",
    "    order_search_results,\n",
    "    update_vector_indexes,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    get_answer,\n",
    ")\n",
    "\n",
    "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_CHAT_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
   "metadata": {},
   "source": [
    "### Let's start with the basics\n",
    "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Tell me some use cases for reinforcement learning?\"\n",
    "FOLLOW_UP_QUESTION = \"Give me the main points of our conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning (RL) has various applications across different domains. Here are some use cases for reinforcement learning:\n",
       "\n",
       "1. Game Playing: RL has excelled in game playing tasks, such as AlphaGo and AlphaZero, where it has achieved superhuman performance in complex board games like Go, chess, and shogi.\n",
       "\n",
       "2. Robotics: RL can be used to train robots for tasks like object manipulation, grasping, locomotion, and navigation in dynamic and uncertain environments.\n",
       "\n",
       "3. Autonomous Vehicles: RL can help in training self-driving cars to make decisions in real-time, such as lane changing, merging, and navigating complex traffic situations.\n",
       "\n",
       "4. Resource Management: RL can optimize resource allocation in areas like energy management, traffic control, and supply chain management, ensuring efficient utilization and minimizing costs.\n",
       "\n",
       "5. Healthcare: RL can be applied in personalized medicine, treatment optimization, and clinical decision-making, helping doctors determine the best treatment plans for patients.\n",
       "\n",
       "6. Recommender Systems: RL can be used to build recommendation engines that learn user preferences and make personalized recommendations for movies, music, products, and more.\n",
       "\n",
       "7. Finance: RL can assist in portfolio management, algorithmic trading, and risk management by learning optimal investment strategies and adapting to market dynamics.\n",
       "\n",
       "8. Industrial Control: RL can optimize control systems in manufacturing, energy, and process industries, improving efficiency, reducing energy consumption, and minimizing waste.\n",
       "\n",
       "9. Natural Language Processing: RL can be applied to dialogue systems and conversational agents, enabling them to learn how to respond to user queries and carry out meaningful conversations.\n",
       "\n",
       "10. Education: RL can be used to create intelligent tutoring systems that adapt to individual student needs, providing personalized instruction and feedback.\n",
       "\n",
       "These are just a few examples, and RL has the potential to be applied in various other domains to solve complex decision-making problems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "response = chain.run(QUESTION)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I cannot recall the details of our conversation as I am an AI language model and do not have the capability to remember past interactions. However, if you provide me with specific information or questions, I'll be happy to assist you.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "chain.run(FOLLOW_UP_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As you can see, it doesn't remember what it just responded, sometimes it responds based only on the system prompt, or just randomly. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "                {history}\n",
    "                Human: {question}\n",
    "                AI:\n",
    "            \"\"\"\n",
    "    )\n",
    "chain = LLMChain(llm=llm, prompt=hist_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Reinforcement learning (RL) has various applications across different domains.\n",
       "- Some use cases for RL include game playing, robotics, autonomous vehicles, resource management, healthcare, recommender systems, finance, industrial control, natural language processing, and education.\n",
       "- RL has achieved superhuman performance in game playing tasks like Go, chess, and shogi.\n",
       "- RL can be used to train robots for tasks like object manipulation, grasping, locomotion, and navigation.\n",
       "- RL can help in training self-driving cars to make real-time decisions in complex traffic situations.\n",
       "- RL can optimize resource allocation in areas like energy management, traffic control, and supply chain management.\n",
       "- RL can be applied in personalized medicine, treatment optimization, and clinical decision-making in healthcare.\n",
       "- RL can be used to build recommendation engines for personalized recommendations.\n",
       "- RL can assist in portfolio management, algorithmic trading, and risk management in finance.\n",
       "- RL can optimize control systems in manufacturing, energy, and process industries.\n",
       "- RL can be applied to dialogue systems and conversational agents in natural language processing.\n",
       "- RL can be used to create intelligent tutoring systems for personalized instruction and feedback in education."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain.run({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
   "metadata": {},
   "source": [
    "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
   "metadata": {},
   "source": [
    "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba257e86-fd90-4a51-a72d-27000913e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Memory adds tokens to the prompt, we would need a better model that allows more space on the prompt\n",
    "MODEL = \"gpt-35-turbo-16k\"\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=1000)\n",
    "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name = \"cogsrch-index-files\"\n",
    "index2_name = \"cogsrch-index-csv\"\n",
    "index3_name = \"cogsrch-index-books-vector\"\n",
    "text_indexes = [index1_name, index2_name]\n",
    "vector_indexes = [index+\"-vector\" for index in text_indexes] + [index3_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b01852c2-6192-496c-adff-4270f9380469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 517 ms, sys: 41.1 ms, total: 558 ms\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Search in text-based indexes first and update vector indexes\n",
    "top_k=10\n",
    "agg_search_results = get_search_results(QUESTION, text_indexes, k=top_k, vector_search=False)\n",
    "ordered_results = order_search_results(agg_search_results, k=top_k, reranker_threshold=1, vector_search=False)\n",
    "update_vector_indexes(ordered_search_results=ordered_results, embedder=embedder)\n",
    "\n",
    "# Search in all vector-based indexes available\n",
    "agg_search_results = get_search_results(QUESTION, vector_indexes, k=top_k , vector_search=True, \n",
    "                                        query_vector = embedder.embed_query(QUESTION))\n",
    "top_similarity_k = 5\n",
    "ordered_results = order_search_results(agg_search_results, k=top_similarity_k,\n",
    "                                       vector_search = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca500dd8-148c-4d8a-b58b-2df4c957459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('z4cagypm_0',\n",
       "              {'title': 'Deep reinforcement learning for large-scale epidemic control_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://arxiv.org/pdf/2003.13676v1.pdf',\n",
       "               'caption': 'This experiment shows that deep reinforcement learning can be used to learn mitigation policies in complex epidemiological models with a large state space. Moreover, through this experiment, we demonstrate that there can be an advantage to consider collaboration between districts when designing prevention strategies..\\x00',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': \"Epidemics of infectious diseases are an important threat to public health and global economies. Yet, the development of prevention strategies remains a challenging process, as epidemics are non-linear and complex processes. For this reason, we investigate a deep reinforcement learning approach to automatically learn prevention strategies in the context of pandemic influenza. Firstly, we construct a new epidemiological meta-population model, with 379 patches (one for each administrative district in Great Britain), that adequately captures the infection process of pandemic influenza. Our model balances complexity and computational efficiency such that the use of reinforcement learning techniques becomes attainable. Secondly, we set up a ground truth such that we can evaluate the performance of the 'Proximal Policy Optimization' algorithm to learn in a single district of this epidemiological model. Finally, we consider a large-scale problem, by conducting an experiment where we aim to learn a joint policy to control the districts in a community of 11 tightly coupled districts, for which no ground truth can be established. This experiment shows that deep reinforcement learning can be used to learn mitigation policies in complex epidemiological models with a large state space. Moreover, through this experiment, we demonstrate that there can be an advantage to consider collaboration between districts when designing prevention strategies.\",\n",
       "               'score': 0.03333333507180214}),\n",
       "             ('8gaeosyr_0',\n",
       "              {'title': 'A Hybrid Recommendation for Music Based on Reinforcement Learning_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/',\n",
       "               'caption': 'In this paper, we propose a personalized hybrid recommendation algorithm for music based on reinforcement learning (PHRR) to recommend song sequences that match listeners’ preferences better. We firstly use weighted matrix factorization (WMF) and convolutional neural network (CNN) to learn and extract the song feature vectors.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'The key to personalized recommendation system is the prediction of users’ preferences. However, almost all existing music recommendation approaches only learn listeners’ preferences based on their historical records or explicit feedback, without considering the simulation of interaction process which can capture the minor changes of listeners’ preferences sensitively. In this paper, we propose a personalized hybrid recommendation algorithm for music based on reinforcement learning (PHRR) to recommend song sequences that match listeners’ preferences better. We firstly use weighted matrix factorization (WMF) and convolutional neural network (CNN) to learn and extract the song feature vectors. In order to capture the changes of listeners’ preferences sensitively, we innovatively enhance simulating interaction process of listeners and update the model continuously based on their preferences both for songs and song transitions. The extensive experiments on real-world datasets validate the effectiveness of the proposed PHRR on song sequence recommendation compared with the state-of-the-art recommendation approaches.',\n",
       "               'score': 0.032522473484277725}),\n",
       "             ('7sjdzz9x_0',\n",
       "              {'title': 'Balancing Exploration and Exploitation in Self-imitation Learning_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/',\n",
       "               'caption': 'Sparse reward tasks are always challenging in reinforcement learning. Learning such tasks requires both efficient exploitation and exploration to reduce the sample complexity. One line of research called self-imitation learning is recently proposed, which encourages the agent to do more exploitation by imitating past good trajectories.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'Sparse reward tasks are always challenging in reinforcement learning. Learning such tasks requires both efficient exploitation and exploration to reduce the sample complexity. One line of research called self-imitation learning is recently proposed, which encourages the agent to do more exploitation by imitating past good trajectories. Exploration bonuses, however, is another line of research which enhances exploration by producing intrinsic reward when the agent visits novel states. In this paper, we introduce a novel framework Explore-then-Exploit (EE), which interleaves self-imitation learning with an exploration bonus to strengthen the effect of these two algorithms. In the exploring stage, with the aid of intrinsic reward, the agent tends to explore unseen states and occasionally collect high rewarding experiences, while in the self-imitating stage, the agent learns to consistently reproduce such experiences and thus provides a better starting point for subsequent stages. Our result shows that EE achieves superior or comparable performance on variants of MuJoCo environments with episodic reward settings.',\n",
       "               'score': 0.03226646035909653}),\n",
       "             ('r253ygx0_0',\n",
       "              {'title': 'Cross-data Automatic Feature Engineering via Meta-learning and Reinforcement Learning_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/',\n",
       "               'caption': 'CAFEM contains two components: a FE learner (FeL) that learns fine-grained FE strategies on one single dataset by Double Deep Q-learning (DDQN) and a Cross-data Component (CdC) that speeds up FE learning on an unseen dataset by the generalized FE policies learned by Meta-Learning on a collection of datasets.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'Feature Engineering (FE) is one of the most beneficial, yet most difficult and time-consuming tasks of machine learning projects, and requires strong expert knowledge. It is thus significant to design generalized ways to perform FE. The primary difficulties arise from the multiform information to consider, the potentially infinite number of possible features and the high computational cost of feature generation and evaluation. We present a framework called Cross-data Automatic Feature Engineering Machine (CAFEM), which formalizes the FE problem as an optimization problem over a Feature Transformation Graph (FTG). CAFEM contains two components: a FE learner (FeL) that learns fine-grained FE strategies on one single dataset by Double Deep Q-learning (DDQN) and a Cross-data Component (CdC) that speeds up FE learning on an unseen dataset by the generalized FE policies learned by Meta-Learning on a collection of datasets. We compare the performance of FeL with several existing state-of-the-art automatic FE techniques on a large collection of datasets. It shows that FeL outperforms existing approaches and is robust on the selection of learning algorithms. Further experiments also show that CdC can not only speed up FE learning but also increase learning performance.',\n",
       "               'score': 0.031054403632879257}),\n",
       "             ('f3oswivw_0',\n",
       "              {'title': 'Data Centers Job Scheduling with Deep Reinforcement Learning_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/',\n",
       "               'caption': 'A2cScheduler consists of two agents, one of which, dubbed the actor, is responsible for learning the scheduling policy automatically and the other one, the critic, reduces the estimation error. Unlike previous policy gradient approaches, A2cScheduler is designed to reduce the gradient estimation variance and to update parameters efficiently.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'Efficient job scheduling on data centers under heterogeneous complexity is crucial but challenging since it involves the allocation of multi-dimensional resources over time and space. To adapt the complex computing environment in data centers, we proposed an innovative Advantage Actor-Critic (A2C) deep reinforcement learning based approach called A2cScheduler for job scheduling. A2cScheduler consists of two agents, one of which, dubbed the actor, is responsible for learning the scheduling policy automatically and the other one, the critic, reduces the estimation error. Unlike previous policy gradient approaches, A2cScheduler is designed to reduce the gradient estimation variance and to update parameters efficiently. We show that the A2cScheduler can achieve competitive scheduling performance using both simulated workloads and real data collected from an academic data center.',\n",
       "               'score': 0.03102453239262104})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2a3595-c3b7-4376-b9c5-0db7a42b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "top_docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location+os.environ['BLOB_SAS_TOKEN']}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(top_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c26d7540-feb8-4581-849e-003f4bf2a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom token limit for gpt-35-turbo-16k : 14000\n",
      "Combined docs tokens count: 1019\n",
      "Chain Type selected: stuff\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(top_docs)>0):\n",
    "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    num_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
    "    print(\"Custom token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Combined docs tokens count:\",num_tokens)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")\n",
    "    \n",
    "chain_type = \"map_reduce\" if num_tokens > tokens_limit else \"stuff\"  \n",
    "print(\"Chain Type selected:\", chain_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce6efa9-2b8f-4810-904d-5986b4ae0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning can be used in various use cases, including:\n",
       "1. Learning prevention strategies for epidemics of infectious diseases, such as pandemic influenza, in order to automatically learn mitigation policies in complex epidemiological models with a large state space<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "2. Personalized hybrid recommendation algorithm for music based on reinforcement learning, which recommends song sequences that match listeners' preferences better, by simulating the interaction process and continuously updating the model based on preferences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\n",
       "3. Learning sparse reward tasks in reinforcement learning by combining self-imitation learning with exploration bonuses, which enhances both exploitation and exploration to reduce sample complexity<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\n",
       "4. Automatic feature engineering in machine learning projects, where a framework called CAFEM (Cross-data Automatic Feature Engineering Machine) is used to optimize the feature transformation graph and learn fine-grained feature engineering strategies<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\n",
       "5. Job scheduling in data centers using Advantage Actor-Critic (A2C) deep reinforcement learning, where the A2cScheduler agent learns the scheduling policy automatically and achieves competitive scheduling performance<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup>.\n",
       "\n",
       "These use cases demonstrate the versatility of reinforcement learning in solving complex problems and optimizing decision-making processes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 ms, sys: 3.5 ms, total: 43.1 ms\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get the answer\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27501f1b-7db0-4ee3-9cb1-e609254ffa3d",
   "metadata": {},
   "source": [
    "And if we ask the follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf5b323-3b9c-479b-8502-acfc4f7915dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I cannot provide the main points of our conversation as it is not mentioned in the provided extracted parts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_answer(llm=llm, docs=top_docs,  query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fa6e6-226c-400f-a504-30255385f43b",
   "metadata": {},
   "source": [
    "You might get a different response from above, but it doesn't matter what response you get, it will be based on the context given, not on previous answers.\n",
    "\n",
    "Until now we just have the same as the prior Notebook 03: results from Azure Search enhanced by OpenAI model, with no memory\n",
    "\n",
    "**Now let's add memory to it:**\n",
    "\n",
    "Reference: https://python.langchain.com/docs/modules/memory/how_to/adding_memory_chain_multiple_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98b876e-d264-48ae-b5ed-9801d6a9152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has various use cases across different domains. Here are some examples:\n",
       "\n",
       "1. **Epidemic Prevention**: In the context of pandemic influenza, deep reinforcement learning can be used to automatically learn prevention strategies. By constructing epidemiological models and using reinforcement learning techniques, policies can be learned to control the spread of infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "\n",
       "2. **Sparse Reward Tasks**: Reinforcement learning can be challenging when dealing with tasks that have sparse rewards. One approach is self-imitation learning, where the agent imitates past good trajectories to encourage exploitation. Another approach is exploration bonuses, which enhance exploration by providing intrinsic rewards for visiting novel states. A novel framework called Explore-then-Exploit (EE) combines these two approaches to strengthen their effects and achieve better performance in episodic reward settings<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\n",
       "\n",
       "3. **Personalized Music Recommendation**: Reinforcement learning can be applied to personalized music recommendation systems. By using reinforcement learning algorithms, such as weighted matrix factorization and convolutional neural networks, the system can learn and update models based on users' preferences for songs and song transitions. This approach improves the accuracy of song sequence recommendations compared to traditional methods<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\n",
       "\n",
       "4. **Automatic Feature Engineering**: Feature engineering is a crucial task in machine learning projects. A framework called Cross-data Automatic Feature Engineering Machine (CAFEM) uses reinforcement learning to optimize the feature transformation process. By learning fine-grained feature engineering strategies and utilizing meta-learning on a collection of datasets, CAFEM can generate effective features and improve learning performance<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\n",
       "\n",
       "5. **Job Scheduling in Data Centers**: Efficient job scheduling in data centers is crucial for resource allocation. A deep reinforcement learning approach called A2cScheduler uses Advantage Actor-Critic (A2C) to learn scheduling policies automatically. By reducing gradient estimation variance and updating parameters efficiently, A2cScheduler achieves competitive scheduling performance in both simulated and real data center environments<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\")\n",
    "\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf28927b-d9ee-4412-bb07-13e055e832a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points we discussed:\n",
       "\n",
       "1. Reinforcement learning has various use cases across different domains.\n",
       "2. In the context of pandemic influenza, deep reinforcement learning can be used to automatically learn prevention strategies for controlling the spread of infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "3. Sparse reward tasks in reinforcement learning can be challenging. Approaches like self-imitation learning and exploration bonuses can be used to enhance exploration and exploitation<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\n",
       "4. Reinforcement learning can be applied to personalized music recommendation systems to improve the accuracy of song sequence recommendations<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\n",
       "5. Feature engineering is a crucial task in machine learning projects. A framework called CAFEM uses reinforcement learning to optimize the feature transformation process and generate effective features<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\n",
       "6. Efficient job scheduling in data centers can be achieved using deep reinforcement learning approaches like A2cScheduler<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup>.\n",
       "\n",
       "Please let me know if there's anything else I can assist you with."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3830b0b8-0ca2-4d0a-9747-f6273368002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points we discussed:\n",
       "\n",
       "1. Reinforcement learning has various use cases across different domains.\n",
       "2. In the context of pandemic influenza, deep reinforcement learning can be used to automatically learn prevention strategies for controlling the spread of infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "3. Sparse reward tasks in reinforcement learning can be challenging. Approaches like self-imitation learning and exploration bonuses can be used to enhance exploration and exploitation<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\n",
       "4. Reinforcement learning can be applied to personalized music recommendation systems to improve the accuracy of song sequence recommendations<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\n",
       "5. Feature engineering is a crucial task in machine learning projects. A framework called CAFEM uses reinforcement learning to optimize the feature transformation process and generate effective features<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\n",
       "6. Efficient job scheduling in data centers can be achieved using deep reinforcement learning approaches like A2cScheduler<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup>.\n",
       "\n",
       "Please let me know if there's anything else I can assist you with."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e732b-3c8c-4df3-8fcb-c3d01e7bec74",
   "metadata": {},
   "source": [
    "You might get a different answer on the above cell, and it is ok, this bot is not yet well configured to answer any question that is not related to its knowledge base, including salutations.\n",
    "\n",
    "Let's check our memory to see that it's keeping the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1279692c-7eb0-4300-8a66-c7025f02c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning?\\nAI: Reinforcement learning has various use cases across different domains. Here are some examples:\\n\\n1. **Epidemic Prevention**: In the context of pandemic influenza, deep reinforcement learning can be used to automatically learn prevention strategies. By constructing epidemiological models and using reinforcement learning techniques, policies can be learned to control the spread of infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\\n\\n2. **Sparse Reward Tasks**: Reinforcement learning can be challenging when dealing with tasks that have sparse rewards. One approach is self-imitation learning, where the agent imitates past good trajectories to encourage exploitation. Another approach is exploration bonuses, which enhance exploration by providing intrinsic rewards for visiting novel states. A novel framework called Explore-then-Exploit (EE) combines these two approaches to strengthen their effects and achieve better performance in episodic reward settings<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\\n\\n3. **Personalized Music Recommendation**: Reinforcement learning can be applied to personalized music recommendation systems. By using reinforcement learning algorithms, such as weighted matrix factorization and convolutional neural networks, the system can learn and update models based on users\\' preferences for songs and song transitions. This approach improves the accuracy of song sequence recommendations compared to traditional methods<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\\n\\n4. **Automatic Feature Engineering**: Feature engineering is a crucial task in machine learning projects. A framework called Cross-data Automatic Feature Engineering Machine (CAFEM) uses reinforcement learning to optimize the feature transformation process. By learning fine-grained feature engineering strategies and utilizing meta-learning on a collection of datasets, CAFEM can generate effective features and improve learning performance<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\\n\\n5. **Job Scheduling in Data Centers**: Efficient job scheduling in data centers is crucial for resource allocation. A deep reinforcement learning approach called A2cScheduler uses Advantage Actor-Critic (A2C) to learn scheduling policies automatically. By reducing gradient estimation variance and updating parameters efficiently, A2cScheduler achieves competitive scheduling performance in both simulated and real data center environments<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target\\nHuman: Give me the main points of our conversation\\nAI: Based on our conversation, here are the main points we discussed:\\n\\n1. Reinforcement learning has various use cases across different domains.\\n2. In the context of pandemic influenza, deep reinforcement learning can be used to automatically learn prevention strategies for controlling the spread of infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\\n3. Sparse reward tasks in reinforcement learning can be challenging. Approaches like self-imitation learning and exploration bonuses can be used to enhance exploration and exploitation<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\\n4. Reinforcement learning can be applied to personalized music recommendation systems to improve the accuracy of song sequence recommendations<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\\n5. Feature engineering is a crucial task in machine learning projects. A framework called CAFEM uses reinforcement learning to optimize the feature transformation process and generate effective features<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\\n6. Efficient job scheduling in data centers can be achieved using deep reinforcement learning approaches like A2cScheduler<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup>.\\n\\nPlease let me know if there\\'s anything else I can assist you with.\\nHuman: Thank you\\nAI: Based on our conversation, here are the main points we discussed:\\n\\n1. Reinforcement learning has various use cases across different domains.\\n2. In the context of pandemic influenza, deep reinforcement learning can be used to automatically learn prevention strategies for controlling the spread of infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\\n3. Sparse reward tasks in reinforcement learning can be challenging. Approaches like self-imitation learning and exploration bonuses can be used to enhance exploration and exploitation<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\\n4. Reinforcement learning can be applied to personalized music recommendation systems to improve the accuracy of song sequence recommendations<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\\n5. Feature engineering is a crucial task in machine learning projects. A framework called CAFEM uses reinforcement learning to optimize the feature transformation process and generate effective features<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\\n6. Efficient job scheduling in data centers can be achieved using deep reinforcement learning approaches like A2cScheduler<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup>.\\n\\nPlease let me know if there\\'s anything else I can assist you with.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87405173",
   "metadata": {},
   "source": [
    "## Using CosmosDB as persistent memory\n",
    "\n",
    "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wisg to provide recommendations. \n",
    "\n",
    "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
    "We will use a class in LangChain use CosmosDBChatMessageHistory, see [HERE](https://python.langchain.com/en/latest/_modules/langchain/memory/chat_message_histories/cosmos_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7131daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CosmosDB instance from langchain cosmos class.\n",
    "cosmos = CosmosDBChatMessageHistory(\n",
    "    cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "    cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
    "    cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "    connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "    session_id=\"Agent-Test-Session\" + str(random.randint(1, 1000)),\n",
    "    user_id=\"Agent-Test-User\" + str(random.randint(1, 1000))\n",
    "    )\n",
    "\n",
    "# prepare the cosmosdb instance\n",
    "cosmos.prepare_cosmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Memory Object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\",chat_memory=cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27ceb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has various use cases in different domains. Here are some examples:\n",
       "\n",
       "1. **Epidemic Prevention**: Reinforcement learning can be used to automatically learn prevention strategies for infectious diseases. For example, a study used deep reinforcement learning to learn mitigation policies in complex epidemiological models with a large state space<sup>[1]</sup>.\n",
       "\n",
       "2. **Sparse Reward Tasks**: Reinforcement learning can be applied to tasks with sparse rewards. One approach is self-imitation learning, which encourages the agent to exploit past good trajectories. Another approach is exploration bonuses, which enhance exploration by providing intrinsic rewards for visiting novel states. A novel framework called Explore-then-Exploit (EE) interleaves self-imitation learning with an exploration bonus to strengthen the effect of both algorithms<sup>[2]</sup>.\n",
       "\n",
       "3. **Personalized Recommendation Systems**: Reinforcement learning can be used to improve personalized recommendation systems. For example, a personalized hybrid recommendation algorithm for music based on reinforcement learning was proposed. It recommends song sequences that match listeners' preferences better by simulating the interaction process and continuously updating the model based on preferences<sup>[3]</sup>.\n",
       "\n",
       "4. **Feature Engineering**: Reinforcement learning can aid in automating feature engineering, which is a time-consuming task in machine learning projects. A framework called Cross-data Automatic Feature Engineering Machine (CAFEM) formalizes the feature engineering problem as an optimization problem over a Feature Transformation Graph (FTG). CAFEM consists of a FE learner (FeL) and a Cross-data Component (CdC) to speed up feature engineering learning on unseen datasets<sup>[4]</sup>.\n",
       "\n",
       "5. **Job Scheduling**: Reinforcement learning can be used for efficient job scheduling in data centers. An innovative Advantage Actor-Critic (A2C) deep reinforcement learning approach called A2cScheduler was proposed for job scheduling. It consists of two agents, the actor and the critic, which learn the scheduling policy and reduce the estimation error, respectively<sup>[5]</sup>.\n",
       "\n",
       "These are just a few examples of the use cases for reinforcement learning. The applications of reinforcement learning are diverse and continue to expand as researchers explore its potential in various domains.\n",
       "\n",
       "Let me know if there's anything else I can help with!\n",
       "\n",
       "References:\n",
       "[1]<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup> - \"Deep Reinforcement Learning for Autonomous Epidemic Prevention in Complex Epidemiological Models\"\n",
       "[2]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup> - \"Explore-then-Exploit: A Framework for Efficient Exploration in Reinforcement Learning\"\n",
       "[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup> - \"Personalized Hybrid Recommendation Algorithm for Music Based on Reinforcement Learning\"\n",
       "[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing using our Question\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a5ff826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points:\n",
       "\n",
       "1. Reinforcement learning has various use cases in different domains.\n",
       "2. Epidemic prevention: Reinforcement learning can be used to learn prevention strategies for infectious diseases<sup>[1]</sup>.\n",
       "3. Sparse reward tasks: Reinforcement learning can be applied to tasks with sparse rewards, using self-imitation learning and exploration bonuses<sup>[2]</sup>.\n",
       "4. Personalized recommendation systems: Reinforcement learning can improve personalized recommendation systems, such as music recommendations<sup>[3]</sup>.\n",
       "5. Feature engineering: Reinforcement learning can automate feature engineering tasks, speeding up machine learning projects<sup>[4]</sup>.\n",
       "6. Job scheduling: Reinforcement learning can be used for efficient job scheduling in data centers<sup>[5]</sup>.\n",
       "\n",
       "These points provide an overview of the use cases for reinforcement learning. For more details, you can refer to the corresponding references.\n",
       "\n",
       "Let me know if there's anything else I can help with!\n",
       "\n",
       "References:\n",
       "[1]<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup> - \"Deep Reinforcement Learning for Autonomous Epidemic Prevention in Complex Epidemiological Models\"\n",
       "[2]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup> - \"Explore-then-Exploit: A Framework for Efficient Exploration in Reinforcement Learning\"\n",
       "[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup> - \"Personalized Hybrid Recommendation Algorithm for Music Based on Reinforcement Learning\"\n",
       "[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup> - \"Cross-data Automatic Feature Engineering Machine: Learning Feature Engineering for Unseen Datasets\"\n",
       "[5]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup> - \"A2cScheduler: Advantage Actor-Critic Deep Reinforcement Learning for Job Scheduling in Data Centers\"\n",
       "\n",
       "Is there anything else you would like to know?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be1620fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points:\n",
       "\n",
       "1. Reinforcement learning has various use cases in different domains.\n",
       "2. Epidemic prevention: Reinforcement learning can be used to learn prevention strategies for infectious diseases[1].\n",
       "3. Sparse reward tasks: Reinforcement learning can be applied to tasks with sparse rewards, using self-imitation learning and exploration bonuses[2].\n",
       "4. Personalized recommendation systems: Reinforcement learning can improve personalized recommendation systems, such as music recommendations[3].\n",
       "5. Feature engineering: Reinforcement learning can automate feature engineering tasks, speeding up machine learning projects[4].\n",
       "6. Job scheduling: Reinforcement learning can be used for efficient job scheduling in data centers[5].\n",
       "\n",
       "These points provide an overview of the use cases for reinforcement learning. For more details, you can refer to the corresponding references.\n",
       "\n",
       "Let me know if there's anything else I can help with!\n",
       "\n",
       "References:\n",
       "[1][1] - \"Deep Reinforcement Learning for Autonomous Epidemic Prevention in Complex Epidemiological Models\"\n",
       "[2][2] - \"Explore-then-Exploit: A Framework for Efficient Exploration in Reinforcement Learning\"\n",
       "[3][3] - \"Personalized Hybrid Recommendation Algorithm for Music Based on Reinforcement Learning\"\n",
       "[4][4] - \"Cross-data Automatic Feature Engineering Machine: Learning Feature Engineering for Unseen Datasets\"\n",
       "[5][5] - \"A2cScheduler: Advantage Actor-Critic Deep Reinforcement Learning for Job Scheduling in Data Centers\"\n",
       "\n",
       "Is there anything else you would like to know?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5ac98",
   "metadata": {},
   "source": [
    "Let's check our Azure CosmosDB to see the whole conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1d7688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me some use cases for reinforcement learning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Reinforcement learning has various use cases in different domains. Here are some examples:\\n\\n1. **Epidemic Prevention**: Reinforcement learning can be used to automatically learn prevention strategies for infectious diseases. For example, a study used deep reinforcement learning to learn mitigation policies in complex epidemiological models with a large state space<sup>[1]</sup>.\\n\\n2. **Sparse Reward Tasks**: Reinforcement learning can be applied to tasks with sparse rewards. One approach is self-imitation learning, which encourages the agent to exploit past good trajectories. Another approach is exploration bonuses, which enhance exploration by providing intrinsic rewards for visiting novel states. A novel framework called Explore-then-Exploit (EE) interleaves self-imitation learning with an exploration bonus to strengthen the effect of both algorithms<sup>[2]</sup>.\\n\\n3. **Personalized Recommendation Systems**: Reinforcement learning can be used to improve personalized recommendation systems. For example, a personalized hybrid recommendation algorithm for music based on reinforcement learning was proposed. It recommends song sequences that match listeners\\' preferences better by simulating the interaction process and continuously updating the model based on preferences<sup>[3]</sup>.\\n\\n4. **Feature Engineering**: Reinforcement learning can aid in automating feature engineering, which is a time-consuming task in machine learning projects. A framework called Cross-data Automatic Feature Engineering Machine (CAFEM) formalizes the feature engineering problem as an optimization problem over a Feature Transformation Graph (FTG). CAFEM consists of a FE learner (FeL) and a Cross-data Component (CdC) to speed up feature engineering learning on unseen datasets<sup>[4]</sup>.\\n\\n5. **Job Scheduling**: Reinforcement learning can be used for efficient job scheduling in data centers. An innovative Advantage Actor-Critic (A2C) deep reinforcement learning approach called A2cScheduler was proposed for job scheduling. It consists of two agents, the actor and the critic, which learn the scheduling policy and reduce the estimation error, respectively<sup>[5]</sup>.\\n\\nThese are just a few examples of the use cases for reinforcement learning. The applications of reinforcement learning are diverse and continue to expand as researchers explore its potential in various domains.\\n\\nLet me know if there\\'s anything else I can help with!\\n\\nReferences:\\n[1]<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup> - \"Deep Reinforcement Learning for Autonomous Epidemic Prevention in Complex Epidemiological Models\"\\n[2]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup> - \"Explore-then-Exploit: A Framework for Efficient Exploration in Reinforcement Learning\"\\n[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup> - \"Personalized Hybrid Recommendation Algorithm for Music Based on Reinforcement Learning\"\\n[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJ', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Give me the main points of our conversation', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Based on our conversation, here are the main points:\\n\\n1. Reinforcement learning has various use cases in different domains.\\n2. Epidemic prevention: Reinforcement learning can be used to learn prevention strategies for infectious diseases<sup>[1]</sup>.\\n3. Sparse reward tasks: Reinforcement learning can be applied to tasks with sparse rewards, using self-imitation learning and exploration bonuses<sup>[2]</sup>.\\n4. Personalized recommendation systems: Reinforcement learning can improve personalized recommendation systems, such as music recommendations<sup>[3]</sup>.\\n5. Feature engineering: Reinforcement learning can automate feature engineering tasks, speeding up machine learning projects<sup>[4]</sup>.\\n6. Job scheduling: Reinforcement learning can be used for efficient job scheduling in data centers<sup>[5]</sup>.\\n\\nThese points provide an overview of the use cases for reinforcement learning. For more details, you can refer to the corresponding references.\\n\\nLet me know if there\\'s anything else I can help with!\\n\\nReferences:\\n[1]<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup> - \"Deep Reinforcement Learning for Autonomous Epidemic Prevention in Complex Epidemiological Models\"\\n[2]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup> - \"Explore-then-Exploit: A Framework for Efficient Exploration in Reinforcement Learning\"\\n[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup> - \"Personalized Hybrid Recommendation Algorithm for Music Based on Reinforcement Learning\"\\n[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup> - \"Cross-data Automatic Feature Engineering Machine: Learning Feature Engineering for Unseen Datasets\"\\n[5]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup> - \"A2cScheduler: Advantage Actor-Critic Deep Reinforcement Learning for Job Scheduling in Data Centers\"\\n\\nIs there anything else you would like to know?', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Thank you', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Based on our conversation, here are the main points:\\n\\n1. Reinforcement learning has various use cases in different domains.\\n2. Epidemic prevention: Reinforcement learning can be used to learn prevention strategies for infectious diseases[1].\\n3. Sparse reward tasks: Reinforcement learning can be applied to tasks with sparse rewards, using self-imitation learning and exploration bonuses[2].\\n4. Personalized recommendation systems: Reinforcement learning can improve personalized recommendation systems, such as music recommendations[3].\\n5. Feature engineering: Reinforcement learning can automate feature engineering tasks, speeding up machine learning projects[4].\\n6. Job scheduling: Reinforcement learning can be used for efficient job scheduling in data centers[5].\\n\\nThese points provide an overview of the use cases for reinforcement learning. For more details, you can refer to the corresponding references.\\n\\nLet me know if there\\'s anything else I can help with!\\n\\nReferences:\\n[1][1] - \"Deep Reinforcement Learning for Autonomous Epidemic Prevention in Complex Epidemiological Models\"\\n[2][2] - \"Explore-then-Exploit: A Framework for Efficient Exploration in Reinforcement Learning\"\\n[3][3] - \"Personalized Hybrid Recommendation Algorithm for Music Based on Reinforcement Learning\"\\n[4][4] - \"Cross-data Automatic Feature Engineering Machine: Learning Feature Engineering for Unseen Datasets\"\\n[5][5] - \"A2cScheduler: Advantage Actor-Critic Deep Reinforcement Learning for Job Scheduling in Data Centers\"\\n\\nIs there anything else you would like to know?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load message from cosmosdb\n",
    "cosmos.load_messages()\n",
    "cosmos.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
   "metadata": {},
   "source": [
    "![CosmosDB Memory](./images/cosmos-chathistory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
    "\n",
    "We added persitent memory using CosmosDB.\n",
    "\n",
    "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, regardless of the input and it struggles to respond to prompts like: Hello, Thank you, Bye, What's your name, What's the weather and any other task that is not search in the knowledge base.\n",
    "\n",
    "\n",
    "## <u>Important Note</u>:<br>\n",
    "As we proceed, while all the code will remain compatible with GPT-3.5 models, we highly recommend transitioning to GPT-4. Here's why:\n",
    "\n",
    "**GPT-3.5-Turbo** can be likened to a 7-year-old child. You can provide it with concise instructions, but it frequently struggles to follow them accurately. Additionally, its limited memory can make sustained conversations challenging.\n",
    "\n",
    "**GPT-3.5-Turbo-16k** resembles the same 7-year-old, but with an increased attention span for longer instructions. However, it still faces difficulties accurately executing them about half the time.\n",
    "\n",
    "**GPT-4** exhibits the capabilities of a 10-12-year-old child. It possesses enhanced reasoning skills and more consistently adheres to instructions. While its memory retention for instructions is moderate, it excels at following them.\n",
    "\n",
    "**GPT-4-32k** is akin to the 10-12-year-old child with an extended memory. It comprehends lengthy sets of instructions and engages in meaningful conversations. Thanks to its robust memory, it offers detailed responses.\n",
    "\n",
    "Understanding this analogy above will become clearer as you complete the final notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
    "\n",
    "But, does this solve all the possible scenarios that a virtual assistant will require?  **What about if the answer to the Smart Search Engine is not related to text, but instead requires to look into tabular data?** The next notebook explains and solves the tabular problem and the concept of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d9da4-3918-4da6-b235-a3320f0dcb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
